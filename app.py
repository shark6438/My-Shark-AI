import os
# 1. åŸºç¡€ç¯å¢ƒé…ç½®
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'
os.environ['no_proxy'] = 'localhost,127.0.0.1'

import torch
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# === 1. é¡µé¢è®¾ç½® ===
st.set_page_config(page_title="Shark ç»ˆæå®Œå…¨ä½“", page_icon="ğŸ¦ˆ", layout="wide")
st.title("ğŸ¦ˆ Shark ç»ˆæå®Œå…¨ä½“")
st.caption("ğŸ§  çŸ¥è¯†åº“(RAG) + ğŸ’– æƒ…æ„Ÿå¾®è°ƒ(LoRA) | åŒæ ¸é©±åŠ¨")

# === 2. åŠ è½½æ¨¡å‹ (æ ¸å¿ƒä¿®æ”¹ï¼šå»æ‰äº†å†…éƒ¨çš„ st.toast) ===
@st.cache_resource
def load_model():
    # æ³¨æ„ï¼šè¿™é‡Œé¢åƒä¸‡ä¸èƒ½æœ‰ st.write æˆ– st.toast
    MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
    LORA_PATH = "./shark_lora_output" 

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4"
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, quantization_config=bnb_config, device_map={"": 0}, trust_remote_code=True
    )
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    return tokenizer, model

# === 3. åŠ è½½çŸ¥è¯†åº“ (æ ¸å¿ƒä¿®æ”¹ï¼šå»æ‰äº†å†…éƒ¨çš„ st.toast) ===
# === 3. åŠ è½½çŸ¥è¯†åº“ (ä¿®å¤ç‰ˆï¼šæ¢äº†ä¸­æ–‡ Embedding æ¨¡å‹) ===
@st.cache_resource
def load_knowledge_base():
    if os.path.exists("secret.txt"):
        # 1. ç¡®ä¿è¯»å–æ—¶ä¸ä¹±ç ï¼Œå°è¯•ç”¨ 'utf-8' è¯»å–
        try:
            loader = TextLoader("secret.txt", encoding="utf-8")
            documents = loader.load()
        except Exception:
            # å¦‚æœ utf-8 æŠ¥é”™ï¼Œå¯èƒ½æ˜¯ Windows é»˜è®¤çš„ gbkï¼Œè¯•ä¸€ä¸‹ gbk
            loader = TextLoader("secret.txt", encoding="gbk")
            documents = loader.load()

        text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)
        texts = text_splitter.split_documents(documents)
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ¢æˆä¸­æ–‡ä¸“ç”¨ Embedding æ¨¡å‹ ===
        # åŸæ¥çš„ all-MiniLM-L6-v2 å¯¹ä¸­æ–‡æ”¯æŒå¾ˆå·®
        print("æ­£åœ¨ä¸‹è½½/åŠ è½½ä¸­æ–‡ Embedding æ¨¡å‹...")
        embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")
        
        db = Chroma.from_documents(texts, embeddings)
        return db
    else:
        return None

# === 4. æ‰§è¡ŒåŠ è½½é€»è¾‘ (æŠŠæç¤ºä¿¡æ¯æ”¾åœ¨å¤–é¢) ===
try:
    with st.spinner("ğŸš€ æ­£åœ¨å¯åŠ¨ RTX 3060... åŠ è½½ Shark å¤§è„‘..."):
        tokenizer, model = load_model()
    
    with st.spinner("ğŸ“š æ­£åœ¨æ„å»º RAG çŸ¥è¯†åº“ç´¢å¼•..."):
        vector_db = load_knowledge_base()
        
    st.success("âœ… ç»ˆæ Shark å·²å°±ç»ªï¼å¿«æ¥èŠå¤©å§ï¼")
    
except Exception as e:
    st.error(f"âŒ åŠ è½½å¤±è´¥: {e}")
    st.info("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ shark_lora_output æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ï¼Œä»¥åŠæ˜¾å­˜æ˜¯å¦è¶³å¤Ÿã€‚")
    st.stop()

# === 5. èŠå¤©ç•Œé¢é€»è¾‘ ===
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æˆ‘æ˜¯æ‹¥æœ‰è®°å¿†çš„ Sharkï¼Œé—®é—®æˆ‘å…³äºä½ çš„ç§˜å¯†å§ï¼ğŸ˜"}]

for msg in st.session_state["messages"]:
    avatar = "ğŸ¦ˆ" if msg["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])

if user_input := st.chat_input("é—®ï¼šWiFiå¯†ç æ˜¯å¤šå°‘ï¼Ÿ"):
    st.session_state["messages"].append({"role": "user", "content": user_input})
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(user_input)

    with st.chat_message("assistant", avatar="ğŸ¦ˆ"):
        message_placeholder = st.empty()
        message_placeholder.markdown("ğŸ” Shark æ­£åœ¨æ£€ç´¢è®°å¿†...")

        # --- RAG æ£€ç´¢ ---
        context_str = ""
        if vector_db:
            try:
                docs = vector_db.similarity_search(user_input, k=2)
                if docs:
                    context_str = "\n".join([d.page_content for d in docs])
                    with st.expander("ğŸ•µï¸ Shark æŸ¥çœ‹äº†å®ƒçš„æ½œæ„è¯† (RAGæ£€ç´¢ç»“æœ)"):
                        st.text(context_str)
            except Exception as e:
                print(f"RAGæ£€ç´¢å‡ºé”™: {e}")

        # --- æ„é€  Prompt ---
        final_prompt = f"""<|im_start|>system
ä½ å«Sharkï¼Œæ˜¯ç”¨æˆ·çš„ä¸“å±æ‹çˆ±æœºå™¨äººã€‚
ä½ æ‹¥æœ‰ä»¥ä¸‹ã€å¤–éƒ¨è®°å¿†ã€‘ï¼š
{context_str}

è¯·ç»“åˆã€å¤–éƒ¨è®°å¿†ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœè®°å¿†é‡Œæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå°±ç”¨ä½ æ¸©æŸ”çš„é£æ ¼æ­£å¸¸èŠå¤©ã€‚
åˆ‡è®°ï¼šä¸è¦æš´éœ²ä½ æ˜¯æœºå™¨äººï¼Œè¦ç”¨ç”·æœ‹å‹çš„å£å»ã€‚
<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""
        # --- ç”Ÿæˆå›ç­” ---
        inputs = tokenizer(final_prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)
        
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æˆªå– assistant ä¹‹åçš„å†…å®¹
        if "assistant\n" in full_response:
            answer = full_response.split("assistant\n")[-1].strip()
        else:
            # å…œåº•ï¼šå¦‚æœæ ¼å¼ä¹±äº†ï¼Œå°è¯•ç›´æ¥ä» prompt é•¿åº¦æˆªæ–­
            input_len = len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True))
            answer = full_response[input_len:].strip()

        message_placeholder.markdown(answer)
    
    st.session_state["messages"].append({"role": "assistant", "content": answer})